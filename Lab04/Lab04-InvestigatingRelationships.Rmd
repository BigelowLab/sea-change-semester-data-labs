---
title: "Lab 4: Investigating Relationships"
output: 
  html_notebook: default
  github_document: default
always_allow_html: true
---

# Lab Outcomes

1. Use scatter and line plots to visualize relationship between two parameters
2. Use statistics to determine the strength of a relationship (correlation coefficients and linear regressions)

Today we are going to use the `DaRTS_combined_data.csv` file that we made last day. 

# Initializing our R Session

Let's start by setting up our R session. These are good steps to take at the start of any R session.

1. Set the working directory: 
```{r}
setwd("C:/Users/cmitchell/Documents/SeaChangeSemester/2021/Labs/sea-change-semester-data-labs/Lab04/")
```
2. Open a new script
3. Save the new script e.g. `Lab04.R`
4. Import the libraries we'll use today
```{r}
library(ggplot2)
```
5. Import our data
```{r}
DATA <- read.csv("DaRTS_combined_data.csv")
```

# Scatter plots

Scatter plots allow us to quickly understand and visualize relationships.

#### Example: How is chlorophyll fluorescence related to temperature?

```{r, fig.height = 5, fig.width=5}
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  xlim(7,18) + ylim(0,12)
```

## Log-transformations

In the above figure, we have a lot of fluorescence data below 2.5 mg m$^{-3}$ - in fact, the majority of our data are below that value. These data are approximately *log-normally* distributed. This is typical of a lot of geophysical parameters like fluorescence. Hence, we often plot (& perform statistics on) the log-transformed data, so we can better understand the relationship between the parameters.

What is the relationship between temperature and log-transformed fluorescence?

```{r, fig.height = 5, fig.width=5}
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  scale_y_log10(limits=c(0.07,22)) 
```


The above is a great visual tool - but how do we define this relationship scientifically and statistically? We'll look at two different techniques, but often scientists tend to use both of these.

# Correlations between two parameters

In everyday speech, *correlation* is often used interchangeably with relationship and is defined as:

> the state or relation of being correlated; specifically : a relation existing between phenomena or things or between mathematical or statistical variables which tend to vary, be associated, or occur together in a way not expected on the basis of chance alone.

How is correlation defined in a mathematical sense? Using *correlation coefficients*.

## Correlation coefficients

Correlation coefficients can be used to evaluate the direction and the intensity of the relationship between two variables. There are a range of different correlation coefficients, but the most common of these is the *Pearson correlation coefficient*. The Pearson correlation coefficient, often referred to as simply the correlation coefficient, is given the symbol $r$ and describes the linear correlation between two variables. It takes a value between -1 and 1. The sign of the correlation coefficient describes the direction of the correlation e.g. a positive $r$ indicates a positive relationship between the variables. A value of 0 indicates no linear correlation and values further from 0 in either direction (closer to -1 and 1) indicate stronger linear correlations.  See the image below for examples.

<img src="Correlation_examples.png">

[Source: Wikipedia]

#### Example 1: The game "Guess the Correlation"

Try this game: [Guess the Correlation](http://guessthecorrelation.com/)

In this game, players are presented with a stream of scatter plots depicting the relationship between two random variables. The aim is to guess the true Pearson correlation coefficient. Guesses over 0.10 from the right answer are not awarded any points and a life is deducted.

#### Example 2: What is the correlation coefficient between chlorophyll fluorescence and temperature?

Here, we are going to use R to calculate the Pearson's correlation coefficient.

```{r}
# create variables for our two parameters of interest
# NB: we want to the data to be vectors, rather than dataframe columns so we can perform our analysis, hence we need to use the dollar sign, $, to index the DATA dataframe
temp <- DATA$Temperature_C
fl <- DATA$Fluorescence_mg_m3

# calculate the correlation coefficient
corValue <- cor.test(temp, fl, method="pearson")

# displaying the correlation coefficient
corValue
```
The correlation coefficient is the number right at the bottom of the printout, below the word `cor`. In this case, we have a correlation coefficient of 0.336.

But what about all those other bits of information? 

The rest of the information tells us if our correlation is significant or not. There's lots of different tests to use, but in this case, we're using a Students t-test. In the t-test, we are testing the following null and alternative hypotheses:

**H$_0$**: the correlation coefficient is equal to 0

**H$_A$**: the correlation coefficient is not equal to 0

The results of the t-test is a t-score (or t-statistic) and p-value. If the p-value that corresponds to the t-score is less than some threshold (typically 0.05) then we can reject the null hypothesis, and say we have a statistically significant correlation. 

Aside: the t-score and p-value are connected through the t-distribution function. We are not covering those details in this course.

**What about on our log transformed data?**

Here, we need to remove the bad data before doing the correlation calculation.

```{r}
templog <- DATA$Temperature_C
fllog <- log10(DATA$Fluorescence_mg_m3)

# removing the NaNs and Infs
nonans <- is.finite(fllog)
tempnonan <- templog[nonans]
flnonan <- fllog[nonans]

# calculate the correlation coefficient
corValue <- cor.test(tempnonan, flnonan)

# displaying the correlation coefficient
corValue
```

So we end up with a slightly higher correlation coefficient between the log-transformed fluorescence data and temperature (than for the measured fluorescence data and temperature).

#  Linear regressions

The relationship between two variables can be explained through a simple linear regression (here *simple* refers to the fact that two variables are being related). The main outputs from this kind of analysis are a line or curve of best fit and various *summary statistics* or *goodness-of-fit statistics* or *explanatory variables* which define how good the linear regression is. This analysis can be extended to describe relationships between multiple variables via linear multiple regression analysis.

## Best fit line

#### Example: What is the line of best fit between the chlorophyll fluoroescence and temperature?

We need to fit a linear model to our data:
```{r, fig.height = 5, fig.width=5}
# fitting a linear model between the two variables
model1 <- lm(fl ~ temp)

# assigning the slope and intercept from the model to variables
intercept <- model1$coefficients[1]
slope <- model1$coefficients[2]

# adding the line to our plot
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  geom_abline(slope=slope,intercept=intercept, size=2, color = "darkblue") +
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  xlim(7,18) + ylim(0,12)
```

In the above, we used `geom_abline` to add our best fit line, but there are some other ways to do this:

```{r, fig.height = 5, fig.width=5}
# fitting a linear model between the two variables
model1 <- lm(fl ~ temp)

# assigning the slope and intercept from the model to variables
intercept <- model1$coefficients[1]
slope <- model1$coefficients[2]

# adding the line to our plot
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  stat_function(fun = function(x) intercept + slope * x, size = 1, color = 'cyan') +
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  xlim(7,18) + ylim(0,12)

```
We can also add our best fit line equation to the figure:

```{r, fig.height = 5, fig.width=5}
# fitting a linear model between the two variables
model1 <- lm(fl ~ temp)

# assigning the slope and intercept from the model to variables
intercept <- model1$coefficients[1]
slope <- model1$coefficients[2]

# adding the line to our plot: note we've added the line by three different methods
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  stat_function(fun = function(x) intercept + slope * x, size = 1, color = 'cyan') +
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  xlim(7,18) + ylim(0,12) +
  annotate("text", x = 9.5, y = 11, label = paste("Temp = ", format(slope, digits=3), "Fluor + (", format(intercept,digits=3), ")", sep="")) # let's add the equation to the figure

```

**What about on our log-transformed data?**

```{r, fig.height = 5, fig.width=5}
# fitting a linear model between the two variables
modelLog <- lm(flnonan ~ tempnonan)

# assigning the slope and intercept fromt the model to variables
interceptLog <- modelLog$coefficients[1]
slopeLog <- modelLog$coefficients[2]

# adding the line to our plot
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  geom_abline(slope=slopeLog,intercept=interceptLog, size=3, color = "darkblue") +
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  scale_y_log10(limits = c(0.07,13)) +
  annotate("text", x = 10, y = 12.8, label = paste("Temp = ", format(slopeLog, digits=3), " log10(Fluor) + (", format(interceptLog,digits=3), ")", sep="")) # let's add the equation to the figure
```

##### **Important side note about linear regressions**

An important point to note is what makes a regression a *linear* regression. **It is not that you fit a straight line through your data.** We could in fact fit a quadratic or higher order polynomials to our data using linear regression techniques (see below): 

```{r, fig.height = 5, fig.width=6}
# fitting a 2nd and 3rd order polynomial between the two variables
model2 <- lm(fl ~ temp + I(temp^2))

# assigning the coefficients from the models to the variables
p2 <- model2$coefficients[3]
p1 <- model2$coefficients[2]
p0 <- model2$coefficients[1]

# adding the line to our plot: note only one of the above methods works for curves, stat_function
ggplot(data = DATA) + 
  geom_point(mapping = aes(x = Temperature_C, y = Fluorescence_mg_m3)) +
  stat_function(fun = function(x) intercept + slope * x, size = 2, aes(color = '1st order')) +
  stat_function(fun = function(x) p0 + p1 * x + p2 * (x^2), size = 3, aes(color = '2nd order')) +
  scale_colour_manual("",values = c("#a6cee3", "#1f78b4")) + # with Color-hex Color code
  xlab('Temperature (deg C)') +
  ylab('Fluorescence (mg/m^3)') +
  xlim(7,18) + ylim(0,12)

```

What makes it a *linear* regression is the coefficients of the fit are linear. In the above plot, the equation for the quadratic is given by $y=0.036x^2-0.490x+1.700$. This equation is non-linear in the $x$-variable, but linear in the coefficients. A non-linear example is a Michaelis-Menten curve which is used to describe phytoplankton nutrient uptake. It has an equation of the form $y = \frac{\alpha x}{\beta + x}$.

#### **How do linear regressions work?**

Linear regressions typically use a least squares method to determine the best fit line. The method aims to minimise the sum of the squares of the distance between all the data points and the fitted line (the *residuals*). 

## Summary statistics

There are a variety of different statistics which are used to describe how well the linear regression describes the relationship between the two variables. Here, we will consider the following variables:

1. Coefficient of determination, $R^2$ : gives a measure of how much of the variability of the $x$-variable is explained by the $y$-variable. For simple linear regressions which include the intercept term, $R^2=r^2$, i.e. the coefficient of determination is equal to the square of the correlation coefficient.

2. Adjusted $R^2$ : If there are multiple variables or terms included in the relationship (e.g. the quadratic curve has an $x$ term and an $x^2$ term), then the adjusted $R^2$ takes into account only the variables or terms which affect the $y$ variable.

3. $p$-value : describes the probability of the null hypothesis being true. In the case of linear regression analysis, the null hypothesis is there is no relationship between the two variables. If the $p$-value is found to be below a given significance level, typically taken as 0.05 (or 5%), then the null hypothesis can be rejected i.e. there is a statistically significant relationship between the two variables.

4. Residual standard error or the root mean squared error : in R, these are equivalent. The *standard error* is defined to show how far the sample mean is from the population mean. It is calculated by dividing the *standard deviation* by the square root of the number of data points. The *standard deviation* is a  measure of how much the data points differ from the mean. For a linear regression, the *root mean square error* is estimated by taking the square root of the mean of the residuals-squared (or put another way, by dividing the square of the residuals by the number of data points, and taking the square root). By virtue of the least squares fitting procedure used in the linear regression, the mean of the residuals are zero. Hence, mathematically, the root mean square error and the residual standard error are the same, and in this case give an overall idea of how close the data points are to the fitted line.

#### Example: Does the straight line, quadatratic or 3rd order polynomial describe the relationship between chlorophyll fluorescence and temperature best?

Let's print the summary statistics from each of our models (or linear regressions)

```{r}
summary(model1)
```
What we see here is 

1. Some information about the *residuals* (the difference between the predicted value and the measured value i.e. the distance on the y-axis between a data point and the best fit line).
2. The fit coefficients (e.g. the slope and intercept) and associated statistics. Notice here we have a t-score and `Pr`. `Pr` is the p-value for the t-test to check if that coefficient is statistically different from 0.
3. Residual standard error (as discussed above).
4. The R$^2$ and adjusted $R^2$ (as discussed above).
5. The F-statistic and associated p-value. This is a similar test as a t-test, but in the case of a linear regression, looks at the relationship as a whole, not the individual coefficients. Here, the null hypothesis is the model with no independent variables (also known as an intercept-only model) fits the data as well as the regression model. And the alternative hypothesis is the regression model fits the data better than the intercept-only model.

```{r}
summary(model2)
summary(modelLog)
```
If we want to pull the different statistics out of these summaries programmatically we can:
```{r}
# assigning summaries to variables
sum_model1 <- summary(model1)
sum_model2 <- summary(model2)
sum_model3 <- summary(modelLog)

# combining statistics into vectors
rsquare = c(sum_model1$r.squared,sum_model2$r.squared,sum_model3$r.squared)
adjrsquare = c(sum_model1$adj.r.squared,sum_model2$adj.r.squared,sum_model3$adj.r.squared)
std_error = c(sum_model1$sigma,sum_model2$sigma,sum_model3$sigma)

# unfortunately we can't extract the p-value in the same way, but we can calculate it separately
pvalue = c(pf(sum_model1$fstatistic[1],sum_model1$fstatistic[2],sum_model1$fstatistic[3],lower.tail = FALSE),
           pf(sum_model2$fstatistic[1],sum_model2$fstatistic[2],sum_model2$fstatistic[3],lower.tail = FALSE),
           pf(sum_model3$fstatistic[1],sum_model3$fstatistic[2],sum_model3$fstatistic[3],lower.tail = FALSE))


# creating a dataframe containing the model statistics
statsData = data.frame(rsquare,adjrsquare,std_error,pvalue,row.names=c("model1","model2","modelLog"))
statsData
```

## Some additional comments

1. Correlation is not causation. Just because a relationship exists between two variables, it does not mean one causes the other to happen.
2. Linear (or non-linear) regression does not tell the whole story of your data. It will give you an idea of the relationship between different variables, but to understand that relationship, you often need additional research, thought and statistical analysis.

# Relationships between 3 or more variables

In the above we've focused solely on the relationship between two variables. What happens if we want to consider a third, or even fourth variable?

We can use linear model as we did before, but include the other variables. For example, lets see if there is a relationship between chlorophyll fluorescence, temperature and salinity.

```{r}
templog <- DATA$Temperature_C
fllog <- log10(DATA$Fluorescence_mg_m3)
sal <- DATA$Salinity_PSU

# removing the NaNs and Infs
nonans <- is.finite(fllog)
tempnonan <- templog[nonans]
flnonan <- fllog[nonans]
salnonan <- sal[nonans]

modelLog_ts <- lm(flnonan ~ tempnonan + salnonan)

summary(modelLog_ts)
```
We have an extra coefficient, so we end up with the following equation (or model):

$\log10(chlorophyll\  fluorescence) = 0.063\times Temperature - 0.172\times Salinity + 4.745$

This model has increased our R$^2$ values, and reduced the residual standard error compared to when we looked at chlorophyll fluorescence and temperature only.   

What about including day of year as well?

```{r}
templog <- DATA$Temperature_C
fllog <- log10(DATA$Fluorescence_mg_m3)
sal <- DATA$Salinity_PSU
doy <- DATA$doy

# removing the NaNs and Infs
nonans <- is.finite(fllog)
tempnonan <- templog[nonans]
flnonan <- fllog[nonans]
salnonan <- sal[nonans]
doynonan <- doy[nonans]

modelLog_tsdoy <- lm(flnonan ~ tempnonan + salnonan +doynonan)

summary(modelLog_tsdoy)
```
The R$^2$ values, and reduced the residual standard error have improved even more when the day of year is included in our model.